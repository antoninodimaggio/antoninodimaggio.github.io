<!doctype html><html lang=en-us><head><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-168002660-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><title>Predict Vehicle Speed Using Dense Optical Flow | Antonino DiMaggio</title><meta name=title content="Predict Vehicle Speed Using Dense Optical Flow | Antonino DiMaggio"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="chrome=1"><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer-when-downgrade"><base href=https://antoninodimaggio.com><meta name=description content="Predict vehicle speed using dashcam videos and neural networks. Inspired by comma.ai speed challenge."><meta name=author content="Antonino DiMaggio"><meta name=twitter:card content="summary_large_image"><meta name=twitter:site content="@tonyjdimaggio"><meta name=twitter:creator content="@tonyjdimaggio"><meta name=twitter:image content="https://antoninodimaggio.com/images/default_summary_card_large_1440_720.jpg"><meta property="og:title" content="Predict Vehicle Speed Using Dense Optical Flow | Antonino DiMaggio"><meta property="og:type" content="website"><meta property="og:url" content="https://antoninodimaggio.com"><meta property="og:image" content="https://antoninodimaggio.comimages/profile_photo_1080_1080.jpg"><meta name=og:description content="Predict vehicle speed using dashcam videos and neural networks. Inspired by comma.ai speed challenge."><link rel=icon type=image/png sizes=16x16 href=https://antoninodimaggio.com/images/favicon.ico><meta name=theme-color content="#FFF"><link rel=canonical href=https://antoninodimaggio.com/blog/predict-vehicle-speed-using-dense-optical-flow/><link rel=stylesheet href=https://antoninodimaggio.com/style.9082e481ae96d63075cb4af009a2282668ae81d624903ca51008bcb676c7642e.css type=text/css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css></head><body><nav class="row middle-xs center-xs"><div class="col-xs-6 col-sm-1 logo"><a href=https://antoninodimaggio.com/#><img style=border-radius:50% src=https://antoninodimaggio.com/images/profile_photo_1080_1080.jpg alt="Antonino DiMaggio"></a></div><div class="col-xs-3 col-sm-2"><h3><a href=https://antoninodimaggio.com/#about>About</a></h3></div><div class="col-xs-3 col-sm-2"><h3><a href=https://antoninodimaggio.com/#blog>Blog</a></h3></div><div class="col-xs-6 col-sm-1 nav-toggle"><a href class=nav-icon onclick="return false"><img src=https://antoninodimaggio.com/images/icon-menu.png alt="Open Menu" style=width:45px;height:auto>
<img src=https://antoninodimaggio.com/images/icon-x.png alt="Close Menu" style=display:none;width:45px;height:auto></a></div></nav><section class="nav-full row middle-xs center-xs"><div class=col-xs-12><div class="row middle-xs center-xs"><div class=col-xs-12><h1><a href=https://antoninodimaggio.com/#about>About</a></h1></div><div class=col-xs-12><h1><a href=https://antoninodimaggio.com/#blog>Blog</a></h1></div></div></div></section><main><section class="blog container"><h1>Predict Vehicle Speed Using Dense Optical Flow</h1><section class=content><div class=sub-header>August 4, 2020 Â· 6 minutes read<br>Tags:
<a href=https://antoninodimaggio.com/tags/computer-vison>computer vison</a>, <a href=https://antoninodimaggio.com/tags/autonomous-vehicles>autonomous vehicles</a></div><article class=entry-content><p>I stumbled across the <a href=https://github.com/commaai/speedchallenge>comma.ai speed challenge</a> so I decided to give it a shot. There are two dashcam videos: one video is used for training (20,400 frames @ 20 FPS) and the other video is used for testing (10,798 frames @ 20 FPS). The training video is accompanied by the ground truth speed at each frame. The objective is to predict the speed of the test video at each frame. I also managed to acquire a <a href=https://github.com/antoninodimaggio/Voof/blob/master/data/test/test.mp4>third video</a> with the accompanying ground truth speeds which will serve as a test measure to see how well the methods demonstrated can generalize.</p><p>The <a href=https://github.com/antoninodimaggio/Voof>code</a> can be found on my <a href=https://github.com/antoninodimaggio>GitHub</a>.</p><div class=resp-container align=center><iframe class=resp-iframe src=https://www.youtube.com/embed/vko7tUqESHU frameborder=0></iframe>
Training video for the comma.ai speed challenge</div><h2 id=overview>Overview</h2><p>I approached the problem more so as a series of experiments as opposed to an actual attempt to get the lowest MSE on the test set. My approach is pretty simple. First calculate the optical flow field of successive images, then train a CNN on these optical flow fields to predict speed.</p><h2 id=optical-flow>Optical Flow</h2><p><a href=https://en.wikipedia.org/wiki/Optical_flow>Optical flow</a> quantifies the apparent motion of objects between frames. Optical flow can also be defined as the distribution of apparent velocities of movement of brightness patterns in an image. Sparse optical flow constructs flow vectors for &ldquo;interesting features&rdquo; while dense optical flow constructs flow vectors for the whole frame. Sparse optical flow is more computationally efficient but less accurate. I am using dense optical flow which I will refer to as just optical flow from now on. Optical flow has historically been an optimization problem, however neural networks have been shown to work <a href=https://arxiv.org/pdf/1612.01925.pdf>better</a> under certain conditions. Since there are two viable methods to calculate optical flow I decided to try out both. Calculating the relative velocity of the vehicle directly from an optical flow field requires <a href="https://www.youtube.com/watch?v=OB8RncJWIqc">depth</a> (this was not obviously apparent to me at first). In my case the only way to estimate depth would be to use another neural network, which is not a method I chose to explore (although I believe that it may hold promise in terms of generalization).</p><h2 id=video-preprocessing>Video Preprocessing</h2><p>With optical flow estimation in mind, the saturation of each pair of frames was augmented with the same uniform random variable to account for illumination changes that will severely deteriorate performance. This will also help with any overfitting issues that may arise. The majority of the sky and car hood are then cropped since they do not really change between successive frames. The frames are then resized and interpolated to work with the CNN architecture.
<img src=images/predict-vehicle-speed-using-dense-optical-flow/augmented_brightness.png alt="Ilummination change"></p><p><img src=images/predict-vehicle-speed-using-dense-optical-flow/final.png alt="Final transformed image"></p><h2 id=method-1-gunnar-farneback-dense-optical-flow>Method #1: Gunnar-Farneback Dense Optical Flow</h2><p>Gunnar-Farnebeck is an optimization based method for estimating dense optical flow. Two successive frames are preprocessed and fed into the algorithm. The resulting two-dimensional optical flow field can then be turned into a 3 channel RGB image via the following <a href=https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_video/py_lucas_kanade/py_lucas_kanade.html>method</a>.
This process is repeated for each pair of successive frames. The resulting RGB optical flow images are accompanied by their appropriate ground truth speeds and saved as a PyTorch dataset. All of the code for this can be found in <a href=https://github.com/antoninodimaggio/Voof/blob/master/preprocess_farneback.py>preprocess_farneback.py</a>.</p><h2 id=method-2-pwc-net-dense-optical-flow>Method #2: PWC-Net Dense Optical Flow</h2><p>Training data to facilitate supervised learning for optical flow estimation neural networks is hard to come by. Animation turns out to be the solution to this problem since the transformations between each frame can be used as ground truths. I like using PyTorch so I modified this <a href=https://github.com/NVlabs/PWC-Net/tree/master/PyTorch>PWC-Net implementation</a> to work with modern PyTorch. I used the model, provided by the orginal authors, that was pre trained on the <a href=http://sintel.is.tue.mpg.de/>MPI Sintel</a> dataset. PWC-Net is fairly complicated and requires a C++ extension to work with PyTorch, the best way to learn more would be to read the <a href=https://arxiv.org/abs/1709.02371>paper</a>. The procedure is similar to that of Method #1, however the optical flow field is left in two-dimensions, since the output of PWC-Net is slightly different compared to that of Gunnar-Farneback. All of the code for this can be found in <a href=https://github.com/antoninodimaggio/Voof/blob/master/preprocess_pwc.py>preprocess_pwc.py</a>.</p><p><img src=images/predict-vehicle-speed-using-dense-optical-flow/pwc_arch.jpg alt="PWC-Net architecture"></p><div align=center>PWC-Net architecture</div><h2 id=cnn-training>CNN Training</h2><h3 id=cnn-architecture>CNN Architecture</h3><p>Once we have optical flow, we can then attempt to train a convolutional neural network to predict speed. Note that this is a regression task not a classification task (although it would be interesting to explore this problem as such). The CNN architecture is from the <a href=https://developer.nvidia.com/blog/deep-learning-self-driving-cars/>End-to-End Deep Learning for Self-Driving Cars</a> blog post by NVIDIA.</p><p><img src=images/predict-vehicle-speed-using-dense-optical-flow/nvidia_cnn.jpg alt="NVIDIA CNN"></p><div align=center>NVIDIA CNN architecture</div><br>The model was implemented using PyTorch with the following hyperparameters:<ul><li><code>criterion = torch.nn.MSELoss()</code></li><li><code>optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)</code></li><li><code>scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=0, threshold=0.2, threshold_mode='abs', min_lr=1e-8)</code></li></ul><p>I also used Kaiming normal initialization for all of the convolutional layers which I found made the model converge faster.</p><h3 id=training>Training</h3><p>The models for both methods were trained in a very similar fashion. With 80% of the data reserved for training and 20% of the data for evaluation. It is important to note that the data was not randomly shuffled, since this does not preserve integrity between the training and evaluation sets. I noticed that a bunch of previous work randomly shuffled the training and evaluation data, this method will inevitably leak information to the model due to the temporal nature of the dataset (ie. the model can be trained on a frame that came right before a frame in the evaluation dataset), tainting any results that they may have achieved. The Gunnar-Farneback model was trained for 20 epochs while the PWC-Net model was trained for 26 epochs. I found that using learning rate annealing was useful, since the loss tended to stagnate.</p><p><img src=images/predict-vehicle-speed-using-dense-optical-flow/loss.png alt="Loss plot"></p><p>Overall Method #1, using Gunnar-Farneback optical flow, achieved a lower evaluation loss (~12 MSE) compared to that of Method #2 that used PWC-Net to estimate optical flow (~20 MSE). This could solely be due to the fact that I was severely confused on how I could turn the output of PWC-Net into a 3 channel RGB representation.</p><div class=resp-container align=center><iframe class=resp-iframe src=https://www.youtube.com/embed/ef5jz3NAdp8 frameborder=0></iframe>
Speeds for demonstration video generated using Method #1</div><h2 id=notes-on-generalization>Notes on Generalization</h2><p><code>The speed challenge states that an MSE of &lt;10 is good. &lt;5 is better. &lt;3 is heart.</code> I can firmly state that the methods demonstrated in this post do not generalize given the sparse training data that was provided. When I attempt to use the trained models to predict speed on the <a href=https://github.com/antoninodimaggio/Voof/blob/master/data/test/test.mp4>third video</a> I get an MSE between ~70 and ~150. This is most likely due to the fact that the models were only trained on 16,420 frames. The model has simply not seen enough scenarios to generalize. At the start of the <a href=https://www.youtube.com/embed/ef5jz3NAdp8>demonstration video</a> the car drives under an underpass, which in turn is followed by a large error spike. It is clear that the trained model has not seen the scenario in which a car drives under an under pass. However, the model has a super low error on long stretches of open highway, which is what the majority of the training video is composed of. The test video contains a lot of city driving which means a lot of intersections/stop and go traffic. The model was never trained on data like this so why would it be able to make an accurate prediction.</p><p><strong>Put simply, the models need to be trained on a lot more data to gain the ability to generalize.</strong></p><h2 id=previous-work>Previous Work</h2><ul><li><a href=https://github.com/ryanchesler/comma-speed-challenge>https://github.com/ryanchesler/comma-speed-challenge</a></li><li><a href=https://github.com/jovsa/speed-challenge-2017>https://github.com/jovsa/speed-challenge-2017</a></li><li><a href=https://github.com/JonathanCMitchell/speedChallenge>https://github.com/JonathanCMitchell/speedChallenge</a></li></ul><h2 id=conclusion>Conclusion</h2><p>I may expand on this work in the future. There are more <a href=https://github.com/commaai/comma2k19>robust data sets</a> that I would like to explore. Thanks for reading!</p></article><div class=pagination><a href=https://antoninodimaggio.com/blog/introductions-are-in-order/>&#171; Introductions Are In Order</a>
<a href=https://antoninodimaggio.com/blog/integrating-renewables-into-the-grid-with-machine-learning/>Integrating Renewables Into The Grid With Machine Learning &#187;</a></div></section></section></main><footer class="row middle-xs center-xs"><div class="col-xs-3 col-md-2"><a target=_blank rel=noopener href=https://twitter.com/tonyjdimaggio>Twitter</a></div><div class="col-xs-3 col-md-2"><a target=_blank rel=noopener href=https://github.com/antoninodimaggio>GitHub</a></div><div class="col-xs-3 col-md-2"><a target=_blank rel=noopener href=https://linkedin.com/in/antoninodimaggio>LinkedIn</a></div><div class=col-xs-12>Copyright &copy; 2020 Antonino DiMaggio.</div></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js></script><script>hljs.initHighlightingOnLoad();</script><script src=https://antoninodimaggio.com/js/src/main.min.cf082cf479a85d1d096f3ed98a5496d87df6907450b813efe7eb4431cc8b4bbc.js type=text/javascript></script></body></html>